{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHWrhvqONBWd"
      },
      "outputs": [],
      "source": [
        "!pip install langchain transformers sentencepiece accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwIace6sOPsZ",
        "outputId": "28fc3579-2dea-40ac-9c1e-75a950dec8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chinese-LLaMA-Alpaca-2'...\n",
            "remote: Enumerating objects: 1089, done.\u001b[K\n",
            "remote: Counting objects: 100% (452/452), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 1089 (delta 372), reused 340 (delta 332), pack-reused 637\u001b[K\n",
            "Receiving objects: 100% (1089/1089), 8.18 MiB | 28.67 MiB/s, done.\n",
            "Resolving deltas: 100% (675/675), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ymcui/Chinese-LLaMA-Alpaca-2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClQ6kPegOYjv"
      },
      "outputs": [],
      "source": [
        "!pip install hf_transfer\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download --local-dir-use-symlinks False \\\n",
        "--local-dir chinese-alpaca-2-7b hfl/chinese-alpaca-2-7b --exclude *.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LblR5fS0OYqE",
        "outputId": "ba11840b-b1a6-43f6-b986-75f68a4004cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading LLM...\n",
            "2023-09-13 08:10:31.313810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.89s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 1888, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            " \n"
          ]
        }
      ],
      "source": [
        "!cd Chinese-LLaMA-Alpaca-2/scripts/langchain && CUDA_VISIBLE_DEVICES=0 python langchain_sum.py --model_path /content/chinese-alpaca-2-7b --file_path /content/doc.txt --chain_type stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhhUlATSbJOc",
        "outputId": "13f9a53d-f317-4c36-ec65-985ac9b86e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading LLM...\n",
            "2023-09-13 07:23:48.501954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.89s/it]\n",
            " 李白是中国唐代一位著名的诗人，被认为是中国诗歌史上的重要人物之一。他曾经担任过多次官职，但由于桀骜不驯的性格，很快就离开了政府工作岗位。他游历了中国的很多地方并写下了很多诗篇。他的诗歌充满了想象力并且经常使用生动形象的比喻来传达情感。尽管有许多文学作品和典故与他的经历有关，但他本人的具体死亡原因一直是一个谜题。然而，他的才华和诗歌影响了许多之后的诗人和文学家。\n"
          ]
        }
      ],
      "source": [
        "!cd Chinese-LLaMA-Alpaca-2/scripts/langchain && CUDA_VISIBLE_DEVICES=0 python langchain_sum.py --model_path /content/chinese-alpaca-2-7b --file_path /content/doc.txt --chain_type refine"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
